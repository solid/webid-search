name: WebID Crawler

on:
  issues:
    types: [opened, edited]

jobs:
  crawl:
    # Only run if the issue has the 'crawler' label
    if: contains(github.event.issue.labels.*.name, 'crawler')
    runs-on: ubuntu-latest
    
    permissions:
      contents: write
      pull-requests: write
      issues: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
      
      - name: Install dependencies
        run: npm install
      
      - name: Extract WebID from issue
        id: extract
        uses: actions/github-script@v7
        with:
          script: |
            const issueBody = context.payload.issue.body;
            console.log('Issue body:', issueBody);
            
            // Parse the issue body to extract the WebID
            // The format from the issue template is:
            // ### WebID or URL
            // <webid value>
            const webidMatch = issueBody.match(/### WebID or URL\s+([^\s]+)/);
            
            if (!webidMatch) {
              core.setFailed('Could not extract WebID from issue');
              return;
            }
            
            const webid = webidMatch[1].trim();
            console.log('Extracted WebID:', webid);
            
            // Validate it's a URL
            try {
              new URL(webid);
            } catch (error) {
              core.setFailed('Invalid URL: ' + webid);
              return;
            }
            
            core.setOutput('webid', webid);
            
            // Create a sanitized branch name from the WebID
            const branchName = 'crawler/' + webid
              .replace(/https?:\/\//, '')
              .replace(/[^a-zA-Z0-9-]/g, '-')
              .replace(/-+/g, '-')
              .replace(/^-|-$/g, '')
              .substring(0, 50);
            
            core.setOutput('branch', branchName);
      
      - name: Run crawler
        id: crawl
        run: |
          node scripts/crawl.js "${{ steps.extract.outputs.webid }}"
          
          # Check if data/webids.json was modified
          if git diff --quiet data/webids.json; then
            echo "No new WebIDs discovered"
            echo "has_changes=false" >> $GITHUB_OUTPUT
          else
            echo "New WebIDs discovered!"
            echo "has_changes=true" >> $GITHUB_OUTPUT
          fi
      
      - name: Create Pull Request
        if: steps.crawl.outputs.has_changes == 'true'
        uses: peter-evans/create-pull-request@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "Add WebIDs discovered from ${{ steps.extract.outputs.webid }}"
          branch: ${{ steps.extract.outputs.branch }}
          delete-branch: true
          title: "WebIDs discovered from crawling ${{ steps.extract.outputs.webid }}"
          body: |
            ## WebID Crawler Results
            
            This PR adds newly discovered WebIDs from crawling: `${{ steps.extract.outputs.webid }}`
            
            Closes #${{ github.event.issue.number }}
            
            ### Changes
            - Updated `data/webids.json` with newly discovered WebIDs
            
            ---
            *This PR was automatically generated by the WebID crawler workflow.*
          labels: |
            crawler
            automated
      
      - name: Add comment to issue
        if: steps.crawl.outputs.has_changes == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '✅ Crawler completed! A pull request with discovered WebIDs has been created and will automatically close this issue when merged.'
            })
      
      - name: Add comment if no changes
        if: steps.crawl.outputs.has_changes == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'ℹ️ Crawler completed but no new WebIDs were discovered from this URL.'
            })
            
            // Close the issue since there's nothing to do
            github.rest.issues.update({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'closed'
            })
